# Fairness, Explainability, and Interpretability
A repo devoted to providing resources in fairness, explainability, and interprebility in machine learning.

# Tutorials

[Farnadi, G., Liao, Q.V., Creager, E. (2022). *NeurIPS Tutorial: Algorithmic Fairness: At The Intersections*](https://neurips.cc/virtual/2022/tutorial/55815) 

# Workshops

[Lakkaraju H. (2022). *Stanford Workshop: Interpreting Machine Learning Models: State-of-the-art, Challenges, Opportunities*](https://www.youtube.com/playlist?list=PLoROMvodv4rPh6wa6PGcHH6vMG9sEIPxL) [(Slides)](https://docs.google.com/presentation/d/1khY_li29A5aUo_cEVRsvO8pcRn7Xp9Bi/mobilepresent?slide=id.p1)

# Papers

Sorted from the perspective of interpretability, explainability, and fairness.

## Interpretability

[Letham, B., Rudin, C., McCormick, T. H., & Madigan, D. (2015). Interpretable Classifiers using Rules and Bayesian Analysis: Building a Better Stroke Prediction Model. *The Annals of Applied Statistics*](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/Interpretable-classifiers-using-rules-and-Bayesian-analysis--Building-a/10.1214/15-AOAS848.pdf)

[Rudin, C. (2019). Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and use Interpretable Models Instead. *Nature Machine Intelligence*](https://arxiv.org/pdf/1811.10154.pdf)

## Explainability

[Arya, V., Bellamy, R. K., Chen, P. Y., Dhurandhar, A., Hind, M., Hoffman, S. C., ... & Zhang, Y. (2019). One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques. *arXiv preprint arXiv:1909.03012*](https://arxiv.org/pdf/1909.03012.pdf)

[Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A Survey of Methods for Explaining Black Box Models. *ACM Computing Surveys (CSUR)*](https://dl.acm.org/doi/pdf/10.1145/3236009)

[Liao, Q. V., Gruen, D., & Miller, S. (2020). Questioning the AI: Informing Design Practices for Explainable AI User Experiences. *In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems*](https://arxiv.org/pdf/1908.09635.pdf)

[Pawelczyk, M., Agarwal, C., Joshi, S., Upadhyay, S., & Lakkaraju, H. (2022). Exploring Counterfactual Explanations through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis. *In International Conference on Artificial Intelligence and Statistics. PMLR.*](https://arxiv.org/pdf/2106.09992.pdf)

[Ustun, B., Spangher, A., & Liu, Y. (2019). Actionable Recourse in Linear Classification. *In Proceedings of the Conference on Fairness, Accountability, and Transparency*](https://arxiv.org/pdf/1809.06514.pdf)

[Verma, S., Dickerson, J., & Hines, K. (2020). Counterfactual Explanations for Machine Learning: A Review. *arXiv preprint arXiv:2010.10596*](https://arxiv.org/pdf/2010.10596.pdf)

[Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR. Harv. *JL & Tech*](https://jolt.law.harvard.edu/assets/articlePDFs/v31/Counterfactual-Explanations-without-Opening-the-Black-Box-Sandra-Wachter-et-al.pdf)

## Fairness

[Karimi, A. H., Barthe, G., Sch√∂lkopf, B., & Valera, I. (2022). A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations. *ACM Computing Surveys*](https://arxiv.org/pdf/2010.04050.pdf)

[Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., & Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. *ACM Computing Surveys (CSUR)*](https://arxiv.org/pdf/1908.09635.pdf)

[Tran, C., Dinh, M., & Fioretto, F. (2021). Differentially Private Empirical Risk Minimization Under the Fairness Lens. *Advances in Neural Information Processing Systems*](https://proceedings.neurips.cc/paper/2021/file/e7e8f8e5982b3298c8addedf6811d500-Paper.pdf)

[Verma, S., & Rubin, J. (2018, May). Fairness Definitions Explained. *In 2018 IEEE/ACM International Workshop on Software Fairness (Fairware).*](https://vsahil.github.io/files/papers/Fairness_Definitions_Explained_2018.pdf)

# Posts

[Miller, K. (2021) Should AI Models Be Explainable? That Depends. *Standford University*](https://hai.stanford.edu/news/should-ai-models-be-explainable-depends)

[Millidge, B. & Black, S. (2022) The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable. *AI Allignment Forum*](https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight)

# Tools and Libraries

[AI Fairness 360](https://github.com/Trusted-AI/AIF360)

[Fairlearn](https://github.com/fairlearn/fairlearn)

[InterpretML](https://github.com/interpretml/interpret)

%[OpenXAI](https://github.com/AI4LIFE-GROUP/OpenXAI)

[Captum](https://github.com/pytorch/captum)

%[Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)

# Regulations, Laws, Strategies, Guidelines

Several applicable documents regarding AI and ML. This is not a comprehensive list. 

## Canada

[Canadian Human Rights Act](https://laws-lois.justice.gc.ca/eng/acts/h-6/)

[AIDA - Artificial Intelligence and Data Act](https://www.parl.ca/legisinfo/en/bill/44-1/c-27)

## European Union

[Artificial Intelligence Act](https://www.consilium.europa.eu/en/press/press-releases/2022/12/06/artificial-intelligence-act-council-calls-for-promoting-safe-ai-that-respects-fundamental-rights/#:~:text=The%20Council%20has%20adopted%20its,fundamental%20rights%20and%20Union%20values.)

## UK

[National AI Strategy](https://www.gov.uk/government/publications/national-ai-strategy)

# People

[Q. Vera Liao](http://qveraliao.com/)

[Golnoosh Farnadi](https://gfarnadi.github.io/)

[Elliot Creager](https://www.cs.toronto.edu/~creager/)

[Hima Lakkaraju](https://himalakkaraju.github.io/)

[Cynthia Rudin](https://users.cs.duke.edu/~cynthia/)
